{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Modeling with Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will introduce how to deploy a seq2seq model in leveraging the time-series features in the data set. For simplicity of demonstration, we are only using the normalized price features in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS\n",
    "\n",
    "* <a href='00 - DSC 2022 Welcome and Logistics.ipynb#top'>**Section 0. Welcome and Logistics**</a> \n",
    "* <a href='01 - DSC 2022 Problem Definition.ipynb#top'>**Section 1. Problem Definition**</a> \n",
    "* <a href='02 - DSC 2022 Exploratory Data Analysis.ipynb#top'>**Section 2. Exploratory Data Analysis**</a> \n",
    "* <a href='03 - DSC 2022 Hypothesis testing.ipynb#top'>**Section 3. Hypothesis Testing**</a> \n",
    "* <a href='04 - DSC 2022 Feature Engineering.ipynb#top'>**Section 4. Feature Engineering**</a> \n",
    "* <a href='05 - DSC 2022 Modeling.ipynb#top'>**Section 5. Modeling**</a>\n",
    "* <a href='06 - DSC 2022 Modeling with Deep Learning.ipynb#top'>**Section 6. Modeling with Deep Learning**</a>\n",
    "  * [1. Model configuration](#configure)\n",
    "  * [2. Model training](#train)\n",
    "  * [3. Model inference](#inference)\n",
    "* <a href='07 - DSC 2022 Submission.ipynb#top'>**Section 7. Submission**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from feature_engineering import feature_engineering\n",
    "from evaluation import evaluation\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmg = pd.read_excel('cmg.xlsx', index_col = 'offeringId')\n",
    "X_train, X_test, y_train, y_test = feature_engineering(cmg, test_frac = 0.3, normalize = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configure'></a>\n",
    "# 1. Model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with our previous machine learning models is that we disregard the time-sereis characterstics of features in the data set. The predictors (pre15_Price_Normalized, pre14_PriceNormalized, ..., pre1_PriceNormalized) form a time series sequence; the outcomes that we try to predict (post1_Price_Normalized, post7_Price_Normalized, ..., post180_Price_Normalized) also form a sequence. \n",
    "\n",
    "Seq2Seq is a type of Encoder-Decoder model that can convert sequences from one domain to sequences of another domain using recurrent neural network (__RNN__). And Long short-term memory (__LSTM__) is an artificial RNN architecture used in the field of deep learning. The hidden layer output of LSTM includes the hidden state(h) and the memory cell(c). You can find more detailed introduction to RNN <a href='https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks'>**here**</a>. In our problem setting, the input sequence would be normalizd pre-deal prices; and the output sequence is the post-deal returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to configure the model, here we are implementing **seq2seq model with teacher enforcing design**. Teacher forcing is a strategy for training recurrent neural networks that uses ground truth as input, instead of model output from a prior time step as an input. The figure below is a illustration for our model design. \n",
    "\n",
    "Essentially, our model consists of two parts: __encoder__ and __decoder__.  \n",
    "\n",
    "For the encoder part, the pre-deal normalized price of each time step is input into the encoder LSTM cell together with previous cell state c and hidden state h, the process repeats until the last cell state c and hidden state h are generated.\n",
    "\n",
    "For the decoder part, we use the last cell state c and hidden state h from the encoder as the initial states of the decoder LSTM cell. Since we are implementing teacher forcing, the decoder part takes ground truth stock returns as inputs. The decoder outputs hidden state for all the 5 time steps(1, 7, 30, 90, 180), and these hidden states are connected to a dense layer to output the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/seq2seq.png\" width=600 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 32),         4352        ['encoder_inputs[0][0]']         \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 32),   4352        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 32),                      'encoder_lstm[0][1]',           \n",
      "                                 (None, 32)]                      'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 1)     33          ['decoder_lstm[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,737\n",
      "Trainable params: 8,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = 1\n",
    "latent_dimension = 32\n",
    "\n",
    "# the encoder part \n",
    "encoder_inputs= Input(shape=(None, n_features), name = 'encoder_inputs')\n",
    "encoder_lstm=LSTM(latent_dimension, return_state=True, name = 'encoder_lstm') # we only want the output from the last cell \n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# the decoder part \n",
    "decoder_inputs = Input(shape=(None, n_features), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(latent_dimension, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(n_features, name='decoder_dense')\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "\n",
    "# putting them together \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## 2. Model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after configuring our network, we can now go ahead and train our model. LSTM expects input data to be of 3D shape (# observation, timesteps, features). Therefore, we would first need to transform our data into the desired shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_timesteps = 15\n",
    "post_timesteps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for the encoder part \n",
    "X_train_lstm = X_train.filter(like = 'pre').to_numpy().reshape(-1, pre_timesteps, 1)\n",
    "\n",
    "# prepare data for the decoder part \n",
    "y_train_input_lstm = pd.concat([X_train.filter(like = 'pre1_'), y_train.drop(columns = ['post180_Price_Normalized'])], axis = 1)\\\n",
    "                    .to_numpy()\\\n",
    "                    .reshape(-1, post_timesteps, 1)\n",
    "y_train_target_lstm = y_train.copy(deep = True)\\\n",
    "                        .to_numpy()\\\n",
    "                        .reshape(-1, post_timesteps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 51s, sys: 7min 16s, total: 30min 7s\n",
      "Wall time: 10min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2beaaf400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.MeanSquaredError())\n",
    "model.fit([X_train_lstm, y_train_input_lstm], y_train_target_lstm, epochs = 500, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "## 3. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a trained model, our next step is to use the model for inference. However, recall that during the training phase, we fed in ground truth observations into the decoder part. However, in the inference phase, we no longer have the ground truth observations for the test set. The solution would be to predict the output sequence one at a time, that is model output from a prior time step would be an input to the current time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " h state (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " c state (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 32),   4352        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 32),                      'h state[0][0]',                \n",
      "                                 (None, 32)]                      'c state[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 1)     33          ['decoder_lstm[1][0]']           \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the decoder for inference \n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dimension,), name = 'h state')\n",
    "decoder_state_input_c = Input(shape=(latent_dimension,), name = 'c state')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_values = encoder_model.predict(input_seq.reshape(-1, pre_timesteps, 1), verbose = 0)\n",
    "    decoder_inputs = input_seq[-1].reshape(1,1,1)\n",
    "    result = []\n",
    "    \n",
    "    stop_condition = False\n",
    "    while len(result) < post_timesteps:\n",
    "        output, h, c = decoder_model.predict([decoder_inputs] + states_values, verbose = 0)\n",
    "        result.append(output.reshape(-1))\n",
    "        decoder_inputs = output.reshape(1, 1, 1)\n",
    "        states_values = [h, c]\n",
    "    return np.array(result).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq on train set:\n",
      " {'MSE': 9.335408849757176, 'ACC': 0.6357418800972996}\n",
      "CPU times: user 26min 2s, sys: 1min 4s, total: 27min 6s\n",
      "Wall time: 26min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_results = []\n",
    "for i in range(X_train_lstm.shape[0]):\n",
    "    train_results.append(decode_sequence(X_train_lstm[i]))\n",
    "print('Seq2seq on train set:\\n', evaluation(y_train, train_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq on test set:\n",
      " {'MSE': 23.133951086840046, 'ACC': 0.6321094793057369}\n",
      "CPU times: user 10min 52s, sys: 28 s, total: 11min 20s\n",
      "Wall time: 10min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test_lstm = np.array(X_test.filter(like = 'pre')).reshape(-1, pre_timesteps, 1)\n",
    "test_results = []\n",
    "for i in range(X_test_lstm.shape[0]):\n",
    "    test_results.append(decode_sequence(X_test_lstm[i]))\n",
    "print('Seq2seq on test set:\\n', evaluation(y_test, test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example given above, we showcased a very basic seq2seq design. Looks like we overfitted the train data. It is now your turn to design, train and improve your own deep learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
