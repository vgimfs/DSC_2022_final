{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Modeling with Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS\n",
    "\n",
    "* <a href='05- DSC 2022 Modeling .ipynb#top'>**Section 5. Modeling**</a>\n",
    "* <a href='06- DSC 2022 Modeling with deep learning.ipynb#top'>**Section 6 - Modeling with deep learning**</a>\n",
    "  * [1. Model configuration](#configure)\n",
    "  * [2. Model training](#train)\n",
    "  * [3. Model inference](#inference)\n",
    "* <a href='07- DSC 2022 Submission #top'>**Section 7 - Submission**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from feature_engineering import feature_engineering\n",
    "from evaluation import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmg = pd.read_excel('cmg.xlsx', index_col = 'offeringId')\n",
    "X_train, X_test, y_train, y_test = feature_engineering(cmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(true, pred):\n",
    "    \n",
    "    # change format of input if necessary \n",
    "    if isinstance(true, pd.DataFrame) or isinstance(true, list): \n",
    "        true = np.array(true)\n",
    "    if isinstance(pred, pd.DataFrame) or isinstance(pred, list): \n",
    "        pred = np.array(pred)\n",
    "        \n",
    "    # mae \n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    \n",
    "    # direction\n",
    "    n = len(true)\n",
    "    true, pred = (true >= 0), (pred >= 0)\n",
    "    score = 0 \n",
    "    for i in range(n):\n",
    "        score += accuracy_score(true[i], pred[i])\n",
    "    acc = score/n\n",
    "    \n",
    "    return {'MAE':mae, 'ACC': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configure'></a>\n",
    "# 1. Model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to configure the model, here we are implementing seq2seq model with teacher enforcing design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/seq2seq.png\" width=600 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 19:51:36.777266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 32),         4352        ['encoder_inputs[0][0]']         \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 32),   4352        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 32),                      'encoder_lstm[0][1]',           \n",
      "                                 (None, 32)]                      'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 1)     33          ['decoder_lstm[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,737\n",
      "Trainable params: 8,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = 1\n",
    "latent_dimension = 32\n",
    "\n",
    "# the encoder part \n",
    "encoder_inputs= Input(shape=(None, n_features), name = 'encoder_inputs')\n",
    "encoder_lstm=LSTM(latent_dimension, return_state=True, name = 'encoder_lstm') # we only want the output from the last cell \n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# the decoder part \n",
    "decoder_inputs = Input(shape=(None, n_features), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(latent_dimension, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(n_features, name='decoder_dense')\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "\n",
    "# putting them together \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## 2. Model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after configuring our network, we shall go ahead and train our model. LSTM expects input data to be of 3D shape (# observation, timesteps, features). In our case, the number of timesteps is 15 since we are using data from 15 days prior to deal announcement; and the number of features if 1 since we are using just the stock price feature. Of course we could include more features, if they are not time series, then we could just replicate its values for # time steps. For the sake of a easy tutorial, we shall only work with the stock price feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_timesteps = 15\n",
    "post_timesteps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for the encoder part \n",
    "X_train_lstm = X_train.filter(like = 'pre').to_numpy().reshape(-1, pre_timesteps, 1)\n",
    "\n",
    "# prepare data for the decoder part \n",
    "y_train_input_lstm = pd.concat([X_train.filter(like = 'pre1_'), y_train.drop(columns = ['post180_Price_Normalized'])], axis = 1)\\\n",
    "                    .to_numpy()\\\n",
    "                    .reshape(-1, post_timesteps, 1)\n",
    "y_train_target_lstm = y_train.copy(deep = True)\n",
    "                        .to_numpy()\\\n",
    "                        .reshape(-1, post_timesteps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbda4a99130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.MeanAbsoluteError())\n",
    "model.fit([X_train_lstm, y_train_input_lstm], y_train_target_lstm, epochs = 500, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "## 3. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a trained model, our next step is to use the model for inference. However, recall that during the training phase, we fed in true observations into the decoder part. However, in the inference phase, we no longer have the true observations. The solution to that is to predict output one at a time.\n",
    "The inputs that the decoder takes are hidden state & cell state from the last lstm cell in the encoder, as well as $X_(-1)$\n",
    "\n",
    "1) Encode the input sentence and retrieve the initial decoder state  \n",
    "2) Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.  \n",
    "3) Append the target character predicted and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " h state (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " c state (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 32),   4352        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 32),                      'h state[0][0]',                \n",
      "                                 (None, 32)]                      'c state[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, None, 1)     33          ['decoder_lstm[3][0]']           \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the decoder for inference \n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dimension,), name = 'h state')\n",
    "decoder_state_input_c = Input(shape=(latent_dimension,), name = 'c state')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_values = encoder_model.predict(input_seq.reshape(-1, pre_timesteps, 1))\n",
    "    decoder_inputs = input_seq[-1].reshape(1,1,1)\n",
    "    result = []\n",
    "    \n",
    "    stop_condition = False\n",
    "    while len(result) < post_timesteps:\n",
    "        output, h, c = decoder_model.predict([decoder_inputs] + states_values, verbose = 0)\n",
    "        result.append(output.reshape(-1))\n",
    "        decoder_inputs = output.reshape(1, 1, 1)\n",
    "        states_values = [h, c]\n",
    "    return np.array(result).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq on train set:\n",
      " {'MAE': 0.5060702792890398, 'ACC': 0.6481445120754007}\n"
     ]
    }
   ],
   "source": [
    "train_results = []\n",
    "for i in range(X_train_lstm.shape[0]):\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "    train_results.append(decode_sequence(X_train_lstm[i]))\n",
    "print('Seq2seq on train set:\\n', evaluation(y_train, train_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq on test set:\n",
      " {'MAE': 0.5903408798393653, 'ACC': 0.6249705535924588}\n"
     ]
    }
   ],
   "source": [
    "X_test_lstm = np.array(X_test.filter(like = 'pre')).reshape(-1, pre_timesteps, 1)\n",
    "test_results = []\n",
    "for i in range(X_test_lstm.shape[0]):\n",
    "    test_results.append(decode_sequence(X_test_lstm[i]))\n",
    "print('Seq2seq on test set:\\n', evaluation(y_test, test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example given above, we showcased a very basic seq2seq model. There are quite a lot aspects left for you to improve: \n",
    "\n",
    "1. Model configuration\n",
    "2. Training \n",
    "3. Include non time series features "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
