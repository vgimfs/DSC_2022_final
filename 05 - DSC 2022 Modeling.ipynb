{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be showing examples of building machine learning models, improving model performances through hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS\n",
    "* <a href='04 - DSC 2022 Feature Engineering.ipynb#top'>**Section 4. Feature Engineering**</a> \n",
    "* <a href='05 - DSC 2022 Modeling.ipynb#top'>**Section 5. Modeling**</a>\n",
    "  * [1. Machine learning](#ml)\n",
    "  * [2. Improving model performance](#improve)\n",
    "* <a href='06 - DSC 2022 Modeling with Deep Learning.ipynb#top'>**Section 6. Modeling with Deep Learning**</a>\n",
    "* <a href='07 - DSC 2022 Submission.ipynb#top'>**Section 7. Submission**</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **best model performance**, the metric we will measure you on is __[Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)__(__MSE__). The equation is:\n",
    "\\begin{equation*}\n",
    "MSE   = \\frac{\\sum_{i = 1}^{N}MSE_i}{N}, \\text{where N is the number of observations}.\n",
    "\\end{equation*} That is, the MSE score is the average MSE you get across all observations. The lower the MSE, the better the model.\n",
    "\n",
    "    For each obersevation $i$, $MSE_i$ is calculated as \n",
    "\\begin{equation*}\n",
    "MSE_i   = \\frac{\\sum_{t=1, 7, 30, 90, 180}(y^t_{i,true}-y^t_{i,pred})^2}{5}.\n",
    "\\end{equation*} \n",
    "\n",
    "- For **best model explainability**, we are looking for models that best answer the initial hypotheses and make most sense from the business side. Our evaluation function returns an **accuracy score** other than MSE. In reality, we care more about whether we get the direction of returns right(that is, the sign of returns). The metric accuracy is designed for measuring how well your model catches that directions. The higher the accuracy, the better the model. \n",
    "\n",
    "You can find the evaluation function in **evaluation.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "from feature_engineering import *\n",
    "from evaluation import evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 6989 observations\n",
      "Test set has 2996 observations\n"
     ]
    }
   ],
   "source": [
    "cmg = pd.read_excel('cmg.xlsx', index_col = 'offeringId')\n",
    "X_train, X_test, y_train, y_test = feature_engineering(cmg, test_frac = 0.3)\n",
    "print('Train set has {} observations'.format(X_train.shape[0]))\n",
    "print('Test set has {} observations'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to fitting any models, it is import to develop a baseline. If our model cannot beat a dummy baseline, then we might have a problem!Let's develop a dummy baseline by guessing post deal returns to be the average of returns in the training set respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 15.615496710489984, 'ACC': 0.6514808985548788}\n",
      "{'MSE': 25.39174798757925, 'ACC': 0.6447263017356438}\n"
     ]
    }
   ],
   "source": [
    "print(evaluation(y_train.to_numpy(), np.repeat(y_train.mean().to_numpy().reshape(1, -1), y_train.shape[0], axis = 0)))\n",
    "print(evaluation(y_test.to_numpy(), np.repeat(y_train.mean().to_numpy().reshape(1, -1), y_test.shape[0], axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "## 1. Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from the easiest regression model - linear regression. Looks like the linear regression model is a little bit better than our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 9.604357631932416, 'ACC': 0.6125339819716721}\n",
      "{'MSE': 23.53859908896831, 'ACC': 0.5990654205607456}\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "print(evaluation(y_train.to_numpy(), lr_model.predict(X_train)))\n",
    "print(evaluation(y_test.to_numpy(), lr_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on train set:\n",
      " {'MSE': 8.624586397283258, 'ACC': 0.7299470596652045}\n",
      "KNN on test set:\n",
      " {'MSE': 23.893491749984697, 'ACC': 0.6489986648865107}\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsRegressor().fit(X_train, y_train)\n",
    "print('KNN on train set:\\n', evaluation(y_train.to_numpy(), knn_model.predict(X_train)))\n",
    "print('KNN on test set:\\n', evaluation(y_test.to_numpy(), knn_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree methods \n",
    "\n",
    "The decision tree regressor breaks down a dataset into smaller and smaller subsets while at the same time a decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. \n",
    "\n",
    "How do we make splits then? The default function in sklearn to measure the quality of a split is Mean Squared Error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node. You can read more [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree on train set:\n",
      " {'MSE': 4.962948389259151, 'ACC': 0.9446272714265321}\n",
      "Decision tree on test set:\n",
      " {'MSE': 26.04377182530149, 'ACC': 0.6652202937249643}\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeRegressor(random_state=0).fit(X_train, y_train)\n",
    "print('Decision tree on train set:\\n', evaluation(y_train.to_numpy(), tree_model.predict(X_train)))\n",
    "print('Decision tree on test set:\\n', evaluation(y_test.to_numpy(), tree_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='improve'></a>\n",
    "## 2. Improving model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways in how we could improve model performances. For example, \n",
    "\n",
    "- Try different models \n",
    "- Feature engineering \n",
    "- Regularization\n",
    "- Hyperparameter tuning through cross validation\n",
    "...\n",
    "\n",
    "We will provide code examples for **hyperparameter tuning through cross validation** in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that using decision tree regressor, the models performs much better on the train set than the test set. Such a problem is called **overfitting** and is common in practice. The reason for overfitting is that the model has learnt from the train set too good to an extent that the model cannot be generalized to other data. The figure below is a great illustration for underfitting and overfitting. \n",
    "\n",
    "How do we know we have overfit the train data? An alarming signal is high train performance but low test performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/overfit.png\" width=600 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we solve overfitting? If we cannot get a bigger and richer data set, we could make the model 'simpler', that is, to decrease the complexity of the model. Using decision tree regressor as an example, we could decrease the max depth of the tree so that we get a smaller tree by stopping early. \n",
    "\n",
    "But then you might come up with the question: how shall I decide the optimal depth? The answer to your question is through **cross-validation**. Say we are doing k-fold cross validation. Then we are splitting the data into k groups, and each time we fit the Split the dataset into k groups. Each group will be used as the test set once while a model with specified hyperparameters is fitted on the remaning k-1 groups; the model performance would be evaluted on the test set. **The aim of cross validation is not to fit a bunch of models. But instead is for us to find the best hyperparameters**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/cv.png\" width=600 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes below is an exmpale of 5-fold cross validtion, as illustrated in the figure above. \n",
    "Here, instead of normalizing the entire training set using the feature_engineering function we created in section 4, we wrap up normalization and model into a single pipeline and pass it into GridSearchCV. The reason is that each fold(group) will now be used as a test set in cross validation, and it should be normalized based on the mean & variance of other 4 folds, instead of based on the mean & varaince of the entire training set. \n",
    "\n",
    "In our example, the hyperparameter that we are trying to tune is max depth of a decision tree; the values we are trying out are 10, 50 and 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = feature_engineering(cmg, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor \n",
    "numerical_cols = list(X_train.select_dtypes(include=np.number))\n",
    "categorical_cols = [col for col in list(X_train) if col not in numerical_cols]\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "# model \n",
    "model = DecisionTreeRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('pre',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['offeringDiscountToLastTrade',\n",
       "                                                                          'offeringPrice',\n",
       "                                                                          'pre15_Price_Normalized',\n",
       "                                                                          'pre14_Price_Normalized',\n",
       "                                                                          'pre13_Price_Normalized',\n",
       "                                                                          'pre12_Price_Normalized',\n",
       "                                                                          'pre11_Price_Normalized',\n",
       "                                                                          'pre10_Price_Normalized',\n",
       "                                                                          'pre9_Price_Normalized',\n",
       "                                                                          'pre8_Price_Normalized...\n",
       "                                                                          'pre4_Price_Normalized',\n",
       "                                                                          'pre3_Price_Normalized',\n",
       "                                                                          'pre2_Price_Normalized',\n",
       "                                                                          'pre1_Price_Normalized',\n",
       "                                                                          'totalBookrunners']),\n",
       "                                                                        ('cat',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['offeringType',\n",
       "                                                                          'offeringSector',\n",
       "                                                                          'changeBank'])])),\n",
       "                                       ('model',\n",
       "                                        DecisionTreeRegressor(random_state=0))]),\n",
       "             n_jobs=1, param_grid={'model__max_depth': [10, 50, 100]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting everything together \n",
    "pipe = Pipeline(steps=[(\"pre\", preprocessor), (\"model\", model)])\n",
    "param_grid = {\n",
    "    'model__max_depth':[10, 50, 100]}\n",
    "tree_model_cv = GridSearchCV(pipe, param_grid, n_jobs=1, scoring='neg_mean_squared_error', cv = 5, refit = True)\n",
    "tree_model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the results from cross validation, as explained above, since we are doing 5-fold cross validtion, the train set is split into 5 groups. For each hyperparameter combination(here, we are just tuning one single hyperparameter, max_depth), each group is used as the test set for a model with the specified hyperparameter combination fitted on the other four groups. And then we get a mean test score for each hyperparmeter combination by taking the average over test scores evaluated on the five groups. We would choose max_depth = 10 since it has the lowest mean test score in cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079575</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>10</td>\n",
       "      <td>{'model__max_depth': 10}</td>\n",
       "      <td>-37.313538</td>\n",
       "      <td>-28.952039</td>\n",
       "      <td>-10.745832</td>\n",
       "      <td>-8.398568</td>\n",
       "      <td>-19.315139</td>\n",
       "      <td>-20.945023</td>\n",
       "      <td>10.919104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195951</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.006299</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>50</td>\n",
       "      <td>{'model__max_depth': 50}</td>\n",
       "      <td>-38.773212</td>\n",
       "      <td>-29.021863</td>\n",
       "      <td>-12.560251</td>\n",
       "      <td>-9.632726</td>\n",
       "      <td>-19.919786</td>\n",
       "      <td>-21.981567</td>\n",
       "      <td>10.742729</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.192644</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>100</td>\n",
       "      <td>{'model__max_depth': 100}</td>\n",
       "      <td>-38.519847</td>\n",
       "      <td>-27.760295</td>\n",
       "      <td>-13.133614</td>\n",
       "      <td>-9.631510</td>\n",
       "      <td>-18.821158</td>\n",
       "      <td>-21.573285</td>\n",
       "      <td>10.458234</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.079575      0.004995         0.006127        0.000575   \n",
       "1       0.195951      0.009761         0.006299        0.000529   \n",
       "2       0.192644      0.014100         0.006214        0.000746   \n",
       "\n",
       "  param_model__max_depth                     params  split0_test_score  \\\n",
       "0                     10   {'model__max_depth': 10}         -37.313538   \n",
       "1                     50   {'model__max_depth': 50}         -38.773212   \n",
       "2                    100  {'model__max_depth': 100}         -38.519847   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -28.952039         -10.745832          -8.398568         -19.315139   \n",
       "1         -29.021863         -12.560251          -9.632726         -19.919786   \n",
       "2         -27.760295         -13.133614          -9.631510         -18.821158   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -20.945023       10.919104                1  \n",
       "1       -21.981567       10.742729                3  \n",
       "2       -21.573285       10.458234                2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tree_model_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning, our decision tree model performs much better on the test set in terms of MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree tuned on train set:\n",
      " {'MSE': 10.917166645380743, 'ACC': 0.6733600400601004}\n",
      "Decision tree tuned on test set:\n",
      " {'MSE': 14.739607806319494, 'ACC': 0.6636955433149685}\n"
     ]
    }
   ],
   "source": [
    "print('Decision tree tuned on train set:\\n', evaluation(y_train.to_numpy(), tree_model_cv.predict(X_train)))\n",
    "print('Decision tree tuned on test set:\\n', evaluation(y_test.to_numpy(), tree_model_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
